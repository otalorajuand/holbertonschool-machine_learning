# Attention

>  The project's primary goals are outlined, which include addressing several conceptual questions related to the attention mechanism and its applications in NLP. These questions cover various aspects of attention, from its fundamental definition to its practical implementations in models like transformers. 

At the end of this project I was able to solve these conceptual questions:

* What is the attention mechanism?
* How to apply attention to RNNs
* What is a transformer?
* How to create an encoder-decoder transformer model
* What is GPT?
* What is BERT?
* What is self-supervised learning?
* How to use BERT for specific NLP tasks
* What is SQuAD? GLUE?


## Tasks :heavy_check_mark:

| Filename | Task |
| ------ | ------------------------------------------------- | 
| [0-rnn_encoder.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/0-rnn_encoder.py)| Create a class RNNEncoder that inherits from tensorflow.keras.layers.Layer to encode for machine translation. | 
| [1-self_attention.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/1-self_attention.py)| Create a class SelfAttention that inherits from tensorflow.keras.layers.Layer to calculate the attention for machine translation.  | 
| [2-rnn_decoder.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/2-rnn_decoder.py)| Create a class RNNDecoder that inherits from tensorflow.keras.layers.Layer to decode for machine translation. | 
| [4-positional_encoding.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/4-positional_encoding.py)| Write the function that calculates the positional encoding for a transformer. | 
| [5-sdp_attention.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/5-sdp_attention.py)| Write the function that calculates the scaled dot product attention. | 
| [6-multihead_attention.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/6-multihead_attention.py)| Create a class MultiHeadAttention that inherits from tensorflow.keras.layers.Layer to perform multi head attention. | 
| [7-transformer_encoder_block.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/7-transformer_encoder_block.py)| Create a class EncoderBlock that inherits from tensorflow.keras.layers.Layer to create an encoder block for a transformer. | 
| [8-transformer_decoder_block.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/8-transformer_decoder_block.py)| Create a class DecoderBlock that inherits from tensorflow.keras.layers.Layer to create an encoder block for a transformer. | 
| [9-transformer_encoder.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/9-transformer_encoder.py)| Create a class Encoder that inherits from tensorflow.keras.layers.Layer to create the encoder for a transformer. | 
| [10-transformer_decoder.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/10-transformer_decoder.py)| Create a class Decoder that inherits from tensorflow.keras.layers.Layer to create the decoder for a transformer. | 
| [11-transformer.py](https://github.com/otalorajuand/holbertonschool-machine_learning/blob/main/supervised_learning/attention/11-transformer.py)| Create a class Transformer that inherits from tensorflow.keras.Model to create a transformer network. | 



### Try It On Your Machine :computer:
```bash
git clone https://github.com/otalorajuand/holbertonschool-machine_learning.git
cd supervised_learning/attention
```